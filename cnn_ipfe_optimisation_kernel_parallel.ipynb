{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "c2760c72956b3d3"
  },
  {
   "cell_type": "code",
   "id": "c29d01d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:05:58.241446Z",
     "start_time": "2025-11-12T15:05:57.165440Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:06:05.711486Z",
     "start_time": "2025-11-12T15:05:58.253023Z"
    }
   },
   "cell_type": "code",
   "id": "81bbf67e",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trained CNN",
   "id": "ca69fe0bf0214053"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:06:06.961239Z",
     "start_time": "2025-11-12T15:06:06.856104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda pic: torch.tensor(np.array(pic), dtype=torch.float32).unsqueeze(0))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ],
   "id": "ec44f3165f628cc6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:06:07.000578Z",
     "start_time": "2025-11-12T15:06:06.984255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a lightweight CNN architecture\n",
    "class LightweightCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightweightCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) # stride = 2, padding = 0\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)  # 64 * 1 * 1, 128\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ],
   "id": "2479ac448a3e950c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:06:07.090573Z",
     "start_time": "2025-11-12T15:06:07.081590Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)  # smaller std for large inputs\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ],
   "id": "45027e83a13d95e1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fb3f84ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:06:07.136999Z",
     "start_time": "2025-11-12T15:06:07.112593Z"
    }
   },
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LightweightCNN(num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cpu\n",
      "Total parameters: 98,666\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "fe24be06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:09:05.069245Z",
     "start_time": "2025-11-12T15:06:07.188022Z"
    }
   },
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {100. * correct / total:.2f}%')\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        print(f'Epoch {epoch + 1} completed - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=3)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Batch 0/938, Loss: 2.3615, Accuracy: 14.06%\n",
      "Epoch 1/3, Batch 200/938, Loss: 0.1549, Accuracy: 82.65%\n",
      "Epoch 1/3, Batch 400/938, Loss: 0.0650, Accuracy: 89.01%\n",
      "Epoch 1/3, Batch 600/938, Loss: 0.0829, Accuracy: 91.62%\n",
      "Epoch 1/3, Batch 800/938, Loss: 0.1761, Accuracy: 93.04%\n",
      "Epoch 1 completed - Loss: 0.2204, Accuracy: 93.68%\n",
      "--------------------------------------------------\n",
      "Epoch 2/3, Batch 0/938, Loss: 0.0925, Accuracy: 96.88%\n",
      "Epoch 2/3, Batch 200/938, Loss: 0.1275, Accuracy: 97.52%\n",
      "Epoch 2/3, Batch 400/938, Loss: 0.2304, Accuracy: 97.75%\n",
      "Epoch 2/3, Batch 600/938, Loss: 0.0234, Accuracy: 97.96%\n",
      "Epoch 2/3, Batch 800/938, Loss: 0.0816, Accuracy: 98.02%\n",
      "Epoch 2 completed - Loss: 0.0676, Accuracy: 98.07%\n",
      "--------------------------------------------------\n",
      "Epoch 3/3, Batch 0/938, Loss: 0.0287, Accuracy: 100.00%\n",
      "Epoch 3/3, Batch 200/938, Loss: 0.0254, Accuracy: 98.37%\n",
      "Epoch 3/3, Batch 400/938, Loss: 0.0069, Accuracy: 98.37%\n",
      "Epoch 3/3, Batch 600/938, Loss: 0.0890, Accuracy: 98.43%\n",
      "Epoch 3/3, Batch 800/938, Loss: 0.0483, Accuracy: 98.46%\n",
      "Epoch 3 completed - Loss: 0.0498, Accuracy: 98.52%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3ec1ed97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:09:09.096444Z",
     "start_time": "2025-11-12T15:09:05.158402Z"
    }
   },
   "source": [
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy = test_model(model, test_loader, device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Loss: 0.0454\n",
      "Test Accuracy: 98.63%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "d66e75db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:09:09.851051Z",
     "start_time": "2025-11-12T15:09:09.144486Z"
    }
   },
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, device, num_samples=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        # Move back to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        img = images[0]\n",
    "\n",
    "        img_np = img.squeeze().numpy()  # remove channel dimension\n",
    "\n",
    "        print(\"Pixel values of the first image:\")\n",
    "        print(img_np)\n",
    "\n",
    "        print(f\"Min value: {img_np.min()}, Max value: {img_np.max()}\")\n",
    "\n",
    "        # Create subplot\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Visualizing some predictions...\")\n",
    "visualize_predictions(model, test_loader, device)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing some predictions...\n",
      "Pixel values of the first image:\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  84. 185. 159. 151.  60.  36.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 222. 254. 254. 254. 254. 241. 198. 198.\n",
      "  198. 198. 198. 198. 198. 198. 170.  52.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  67. 114.  72. 114. 163. 227. 254. 225.\n",
      "  254. 254. 254. 250. 229. 254. 254. 140.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.  66.  14.\n",
      "   67.  67.  67.  59.  21. 236. 254. 106.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  83. 253. 209.  18.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  22. 233. 255.  83.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 129. 254. 238.  44.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.  59. 249. 254.  62.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0. 133. 254. 187.   5.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   9. 205. 248.  58.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 126. 254. 182.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   75. 251. 240.  57.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19.\n",
      "  221. 254. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 203.\n",
      "  254. 219.  35.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  38. 254.\n",
      "  254.  77.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 224. 254.\n",
      "  115.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 133. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61. 242. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 254. 219.\n",
      "   40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 207.  18.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Min value: 0.0, Max value: 255.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "a670e23b",
   "metadata": {},
   "source": "## IPFE-enhanced CNN"
  },
  {
   "cell_type": "code",
   "id": "e7a0076c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:09:10.233659Z",
     "start_time": "2025-11-12T15:09:09.858064Z"
    }
   },
   "source": [
    "# import ipfe functions\n",
    "from altered_ipfe import IPFE, decrypt_patches_batch"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### current try",
   "id": "a0e9809d3ef7052c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In the First layer we applied 16 filters of size 3x3, with input channel 1 (grayscale), so the total number of parameters is 16 * 3 * 3 * 1 = 144.\n",
    "\n",
    "So With that each y is one filter flattened to a vector of length 9.\n",
    "For an input x of size 28x28, flattened to a vector of length 784, we can compute the inner product <x, y> for each filter y.\n",
    "\n",
    "Option 1 encrypt the entire input x of length 784, and padd the y to a length of 784 with zeros. -> This will be inefficient as the IPFE scheme will have to handle large vectors.\n",
    "\n",
    "Option 2 encrypt patches of x corresponding to the filter size (3x3 = 9 elements), and compute the inner product for each patch with the filter y. This requires sliding the filter over the input image and encrypting each patch separately.\n",
    "\n",
    "1. Step: Create patches of size 3x3 from the input image (28x28) -> This will create 28x28 patches (with padding). => 784 patches\n",
    "2. Step: Encrypt each patch separately.\n",
    "3. Step: Create 16 query vectors y (one for each filter), each of size 9 (3x3 flattened). => Create a Key for each vector\n",
    "4. Step: decrypt each inner product result to get the convolution output."
   ],
   "id": "e412789862c18ee8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:30.327315Z",
     "start_time": "2025-11-12T15:15:30.318160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os"
   ],
   "id": "823a03b1b98817f6",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def first_conv_forward(self, encrypted_patches, H, W):\n",
    "        num_kernels = len(self.sk_y_array)\n",
    "        num_patches = len(encrypted_patches)\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Output tensor: (num_kernels, num_patches)\n",
    "        decrypted_maps = torch.zeros(num_kernels, num_patches, device=device)\n",
    "\n",
    "        # ----------------------\n",
    "        # Helper function for a single patch\n",
    "        # ----------------------\n",
    "        def process_patch(p_idx, patch, sk_y, y_vec, bias):\n",
    "            val = self.ipfe.new_decrypt(patch, sk_y, y_vec, max_ip=self.prime)\n",
    "            decrypted_val = (val / 10000.0) + bias\n",
    "            return p_idx, decrypted_val\n",
    "\n",
    "        # ----------------------\n",
    "        # Process one kernel at a time\n",
    "        # ----------------------\n",
    "        def process_kernel(k):\n",
    "            sk_y = self.sk_y_array[k]\n",
    "            y_vec = self.y_array[k]\n",
    "            bias = self.biases[k].item()\n",
    "\n",
    "            kernel_results = [0] * num_patches\n",
    "\n",
    "            # Thread across patches\n",
    "            with ThreadPoolExecutor(max_workers=min(num_patches, 8)) as patch_executor:\n",
    "                patch_futures = [patch_executor.submit(process_patch, p_idx, encrypted_patches[p_idx], sk_y, y_vec, bias)\n",
    "                                 for p_idx in range(num_patches)]\n",
    "                for f in patch_futures:\n",
    "                    p_idx, val = f.result()\n",
    "                    kernel_results[p_idx] = val\n",
    "\n",
    "            return k, kernel_results\n",
    "\n",
    "        # ----------------------\n",
    "        # Thread across kernels\n",
    "        # ----------------------\n",
    "        with ThreadPoolExecutor(max_workers=min(num_kernels, 8)) as kernel_executor:\n",
    "            kernel_futures = [kernel_executor.submit(process_kernel, k) for k in range(num_kernels)]\n",
    "            for f in kernel_futures:\n",
    "                k, results = f.result()\n",
    "                decrypted_maps[k, :] = torch.tensor(results, device=device)\n",
    "\n",
    "        # Reshape to (1, num_kernels, H, W)\n",
    "        return decrypted_maps.view(1, num_kernels, H, W)"
   ],
   "id": "ce185fb5e19de4c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:34.549496Z",
     "start_time": "2025-11-12T15:15:34.541206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def first_conv_forward(self, encrypted_patches, H, W):\n",
    "        num_kernels = len(self.sk_y_array)\n",
    "        num_patches = len(encrypted_patches)\n",
    "        device = next(self.parameters()).device\n",
    "\n",
    "        # Output tensor: (num_kernels, num_patches)\n",
    "        decrypted_maps = torch.zeros(num_kernels, num_patches, device=device)\n",
    "\n",
    "        # ----------------------\n",
    "        # Helper function for a single patch\n",
    "        # ----------------------\n",
    "        def process_patch(p_idx, patch, sk_y, y_vec, bias):\n",
    "            val = self.ipfe.new_decrypt(patch, sk_y, y_vec, max_ip=self.prime)\n",
    "            decrypted_val = (val / 10000.0) + bias\n",
    "            return p_idx, decrypted_val\n",
    "\n",
    "        # ----------------------\n",
    "        # Helper function for one kernel\n",
    "        # ----------------------\n",
    "        def process_kernel(k):\n",
    "            sk_y = self.sk_y_array[k]\n",
    "            y_vec = self.y_array[k]\n",
    "            bias = self.biases[k].item()\n",
    "            results = []\n",
    "            for p_idx, patch in enumerate(encrypted_patches):\n",
    "                # Decrypt patch\n",
    "                val = self.ipfe.new_decrypt(patch, sk_y, y_vec, max_ip=self.prime)\n",
    "                decrypted_val = (val / 10000.0) + bias  # scale + bias\n",
    "                results.append(decrypted_val)\n",
    "            return k, results\n",
    "\n",
    "        # ----------------------\n",
    "        # Threaded execution across kernels\n",
    "        # ----------------------\n",
    "        with ThreadPoolExecutor(max_workers=min(num_kernels, 8)) as executor:\n",
    "            futures = [executor.submit(process_kernel, k) for k in range(num_kernels)]\n",
    "            for f in futures:\n",
    "                k, results = f.result()\n",
    "                decrypted_maps[k, :] = torch.tensor(results, device=device)\n",
    "\n",
    "        # Reshape to (1, num_kernels, H, W)\n",
    "        return decrypted_maps.view(1, num_kernels, H, W)"
   ],
   "id": "c3cf955787aac3ce",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:43.083662Z",
     "start_time": "2025-11-12T15:15:43.063830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class IPFECNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, prime=4590007):\n",
    "        super(IPFECNN, self).__init__()\n",
    "        self.prime = prime\n",
    "        self.ipfe = IPFE(prime)\n",
    "        self.encryption_length = 9 # 3x3 filter size flattened\n",
    "\n",
    "        self.ipfe.setup(self.encryption_length)\n",
    "        print(\"IPFE setup done, with length:\", self.encryption_length)\n",
    "\n",
    "        # First convolutional block - this will be used with IPFE\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) # stride = 2, padding = 0\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128) # 64 * 1 * 1, 128\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        #copy weights from the trained model\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        print(\"weights copied from trained model\")\n",
    "\n",
    "        self.weights = self.conv1.weight.data\n",
    "        self.y_array = torch.round(self.weights.view(self.weights.size(0), -1).squeeze(1).view(self.weights.size(0), -1) * 10000).long().tolist()\n",
    "        print(\"weights converted to y vectors\")\n",
    "        self.biases = self.conv1.bias\n",
    "        print(\"biases saved\")\n",
    "        self.sk_y_array = [self.ipfe.key_derive(y) for y in self.y_array]\n",
    "        print(\"sk_ys created\")\n",
    "\n",
    "    def encrypt_data(self, test_set):\n",
    "        unfold = nn.Unfold(kernel_size=3, stride=1, padding=1)\n",
    "        patches = unfold(test_set)\n",
    "        B, patch_size, num_patches = patches.shape\n",
    "\n",
    "        ecnrypted_patches= []\n",
    "\n",
    "        for b in range(B):\n",
    "            patches_b = patches[b].T  # (H*W, patch_size)\n",
    "            encrypted_image = []\n",
    "            ct0_array = np.zeros(num_patches, dtype=np.int64)\n",
    "            cts_array = np.zeros((num_patches, patch_size), dtype=np.int64)\n",
    "\n",
    "            for p_idx in range(num_patches):\n",
    "                patch = patches_b[p_idx]\n",
    "                patch_int = np.array([int(val.item()) % (self.prime - 1) for val in patch], dtype=np.int64)\n",
    "                ct0, ct = self.ipfe.new_encrypt(patch_int)\n",
    "\n",
    "                ct_array = np.array(ct, dtype=np.int64)\n",
    "                encrypted_image.append((ct0, ct_array))\n",
    "\n",
    "            ecnrypted_patches.append(encrypted_image)\n",
    "\n",
    "        return ecnrypted_patches\n",
    "\n",
    "    def first_conv_forward(self, encrypted_image, H, W):\n",
    "        num_kernels = len(self.sk_y_array)\n",
    "        device = next(self.parameters()).device\n",
    "        ct0_array = np.array([ct0 for ct0, _ in encrypted_image], dtype=np.int64)\n",
    "        cts_array = np.stack([ct for _, ct in encrypted_image], axis=0)\n",
    "        num_patches = ct0_array.shape[0]\n",
    "\n",
    "        decrypted_maps = np.zeros((num_kernels, num_patches), dtype=np.float32)\n",
    "\n",
    "        # Helper for one kernel\n",
    "        def decrypt_kernel(k):\n",
    "            sk_y = self.sk_y_array[k]\n",
    "            y_vec = np.array(self.y_array[k], dtype=np.int64)\n",
    "            bias = self.biases[k].item()\n",
    "\n",
    "            decrypted_vals = decrypt_patches_batch(ct0_array, cts_array, sk_y, y_vec, self.ipfe.g, self.prime)\n",
    "            return k, decrypted_vals / 10000.0 + bias\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=min(num_kernels, os.cpu_count() or 4)) as executor:\n",
    "            futures = [executor.submit(decrypt_kernel, k) for k in range(num_kernels)]\n",
    "            for f in futures:\n",
    "                k, vals = f.result()\n",
    "                decrypted_maps[k, :] = vals\n",
    "\n",
    "        # Reshape to CNN feature map\n",
    "        decrypted_tensor = torch.from_numpy(decrypted_maps).float().to(device)\n",
    "        return decrypted_tensor.view(1, num_kernels, H, W)\n",
    "\n",
    "        # Loop over kernels\n",
    "        for k in range(num_kernels):\n",
    "            sk_y = int(self.sk_y_array[k])\n",
    "            y_vec = np.array(self.y_array[k], dtype=np.int64)\n",
    "            bias = float(self.biases[k].item())\n",
    "\n",
    "            # Batch decrypt all patches using Numba\n",
    "            decrypted_vals = decrypt_patches_batch(ct0_array, cts_array, sk_y, y_vec, self.ipfe.g, self.prime)\n",
    "\n",
    "            # Scale and add bias\n",
    "            decrypted_maps[k, :] = torch.tensor(decrypted_vals / 10000.0 + bias, device=device)\n",
    "\n",
    "        # Reshape to (1, num_kernels, H, W)\n",
    "        return decrypted_maps.view(1, num_kernels, H, W)\n",
    "\n",
    "    def forward(self, x, H, W, encrypted=False):\n",
    "        if encrypted:\n",
    "            outputs = []\n",
    "            for sample in x:  # x = [ [patches_img1], [patches_img2], ... ]\n",
    "                feat = self.first_conv_forward(sample, H, W)\n",
    "                feat = self.pool1(F.relu(self.bn1(feat)))\n",
    "                feat = self.pool2(F.relu(self.bn2(self.conv2(feat))))\n",
    "                feat = self.pool3(F.relu(self.bn3(self.conv3(feat))))\n",
    "                feat = feat.view(feat.size(0), -1)\n",
    "                feat = F.relu(self.fc1(feat))\n",
    "                feat = self.dropout(feat)\n",
    "                feat = self.fc2(feat)\n",
    "                outputs.append(feat)\n",
    "            return torch.cat(outputs, dim=0)\n",
    "        else:\n",
    "            x = self.conv1(x)\n",
    "            x = self.pool1(F.relu(self.bn1(x)))\n",
    "            x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "            x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "            x = x.view(x.size(0), -1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.dropout(x)\n",
    "            x = self.fc2(x)\n",
    "            return x\n",
    "\n"
   ],
   "id": "5b0995bba0a5f9bb",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:47.435492Z",
     "start_time": "2025-11-12T15:15:47.424400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initialize IPFE-enhanced CNN\n",
    "# bei n=5\n",
    "\n",
    "# mit kernel 3x3, stride 1, padding 1\n",
    "# 1721257 mit 16 373ms scheint sicher zu sein\n",
    "# 2300003 mit 21 721ms sollte sehr wahrscheinlich sicher zu sein\n",
    "# 4590007 mit 29 470ms muss mathematisch sicher sein\n",
    "\n",
    "# mit kernel 3x3, stride 2, padding 0\n",
    "# 1721257 nur 60%\n",
    "# 2300003 nur 80%\n",
    "# 4590007 mit 5 708ms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ipfe_model = IPFECNN(num_classes=10, prime=4590007).to(device)\n",
    "\n",
    "print(f\"IPFE-CNN model created on device: {device}\")"
   ],
   "id": "48f5b434a96cfcb5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPFE setup done, with length: 9\n",
      "weights copied from trained model\n",
      "weights converted to y vectors\n",
      "biases saved\n",
      "sk_ys created\n",
      "IPFE-CNN model created on device: cpu\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:50.519718Z",
     "start_time": "2025-11-12T15:15:50.513780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def encrypt_test_data(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Encrypt a batch of test data\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Encrypt only a subset\n",
    "        images_subset = images[:num_samples]\n",
    "        labels_subset = labels[:num_samples]\n",
    "\n",
    "        # Encrypt the data\n",
    "        encrypted_data = model.encrypt_data(images_subset)\n",
    "        print(f\"Encrypted {num_samples} samples.\")\n",
    "\n",
    "        H, W = images.size(2), images.size(3)\n",
    "\n",
    "    return encrypted_data, labels_subset, H, W"
   ],
   "id": "f7b0413ef3a19de4",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:54.501925Z",
     "start_time": "2025-11-12T15:15:54.168979Z"
    }
   },
   "cell_type": "code",
   "source": "encrypted_data, labels, H, W = encrypt_test_data(ipfe_model, test_loader, device, num_samples=5)",
   "id": "743adafe19e1c144",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encrypted 5 samples.\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:57.625169Z",
     "start_time": "2025-11-12T15:15:57.618253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Test IPFE functionality with a sample\n",
    "def test_ipfe_cnn(model, encrypted_data, labels, H, W, device):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing IPFE-CNN forward pass on encrypted data...\")\n",
    "        print(f\"Labels of test samples: {labels.cpu().numpy()}\")\n",
    "\n",
    "        try:\n",
    "            outputs = model.forward(encrypted_data, encrypted=True, H=28, W=28) # 13 13\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            print(f\"Predictions on encrypted data: {predicted.cpu().numpy()}\")\n",
    "\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            total = labels.size(0)\n",
    "            print(f\"Accuracy on encrypted samples: {100 * correct / total:.2f}% ({correct}/{total})\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Encrypted IPFE forward pass failed: {e}\")\n"
   ],
   "id": "d06bc5ba89e5cfa6",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:15:59.089325Z",
     "start_time": "2025-11-12T15:15:59.081334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def test_regular_ipfe_cnn(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(\"Testing IPFE-CNN forward pass on encrypted data...\")\n",
    "\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Encrypt only a subset\n",
    "        images_subset = images[:num_samples]\n",
    "        labels_subset = labels[:num_samples]\n",
    "        try:\n",
    "            outputs = model.forward(images_subset, encrypted=False, H=28, W=26) # 13 13\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            print(f\"Predictions on encrypted data: {predicted.cpu().numpy()}\")\n",
    "\n",
    "            correct = (predicted == labels_subset).sum().item()\n",
    "            total = labels_subset.size(0)\n",
    "            print(f\"Accuracy on encrypted samples: {100 * correct / total:.2f}% ({correct}/{total})\")\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Encrypted IPFE forward pass failed: {e}\")"
   ],
   "id": "e83789450ba19902",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:16:39.205627Z",
     "start_time": "2025-11-12T15:16:27.315389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Testing IPFE-CNN functionality...\")\n",
    "test_ipfe_cnn(ipfe_model, encrypted_data, labels, H, W, device)\n"
   ],
   "id": "7c27e8d3d7435993",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IPFE-CNN functionality...\n",
      "Testing IPFE-CNN forward pass on encrypted data...\n",
      "Labels of test samples: [7 2 1 0 4]\n",
      "Predictions on encrypted data: [7 2 1 0 4]\n",
      "Accuracy on encrypted samples: 100.00% (5/5)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-12T15:02:06.029758Z",
     "start_time": "2025-11-12T15:02:06.014432Z"
    }
   },
   "cell_type": "code",
   "source": "test_regular_ipfe_cnn(ipfe_model, test_loader, device, num_samples=5)",
   "id": "af638cf14768fc43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IPFE-CNN forward pass on encrypted data...\n",
      "Predictions on encrypted data: [7 2 1 0 4]\n",
      "Accuracy on encrypted samples: 100.00% (5/5)\n"
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
