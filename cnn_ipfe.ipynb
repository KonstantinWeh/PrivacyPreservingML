{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2760c72956b3d3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29d01d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:40.748350Z",
     "start_time": "2025-10-23T19:04:40.087257Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81bbf67e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.521513Z",
     "start_time": "2025-10-23T19:04:40.976278Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca69fe0bf0214053",
   "metadata": {},
   "source": [
    "## Trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec44f3165f628cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.589824Z",
     "start_time": "2025-10-23T19:04:46.539338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda pic: torch.tensor(np.array(pic), dtype=torch.float32).unsqueeze(0))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f8ff355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.625515Z",
     "start_time": "2025-10-23T19:04:46.616176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a lightweight CNN architecture\n",
    "class LightweightCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightweightCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)  # 28x28 -> 14x14 -> 7x7 -> 3x3 after pooling\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7693a69ca22d4ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.651461Z",
     "start_time": "2025-10-23T19:04:46.645782Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)  # smaller std for large inputs\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3f84ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.687618Z",
     "start_time": "2025-10-23T19:04:46.675074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cpu\n",
      "Total parameters: 98,666\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LightweightCNN(num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe24be06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:05.936613Z",
     "start_time": "2025-10-23T19:04:46.706560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Batch 0/938, Loss: 2.4477, Accuracy: 4.69%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device, epochs)\u001b[39m\n\u001b[32m     13\u001b[39m output = model(data)\n\u001b[32m     14\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     18\u001b[39m running_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {100. * correct / total:.2f}%')\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        print(f'Epoch {epoch + 1} completed - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ec1ed97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.050858Z",
     "start_time": "2025-10-23T19:06:05.969530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Loss: 0.0302\n",
      "Test Accuracy: 99.00%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy = test_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d66e75db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.550819Z",
     "start_time": "2025-10-23T19:06:08.074444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing some predictions...\n",
      "Pixel values of the first image:\n",
      "[[  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  84. 185. 159. 151.  60.  36.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0. 222. 254. 254. 254. 254. 241. 198. 198.\n",
      "  198. 198. 198. 198. 198. 198. 170.  52.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.  67. 114.  72. 114. 163. 227. 254. 225.\n",
      "  254. 254. 254. 250. 229. 254. 254. 140.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  17.  66.  14.\n",
      "   67.  67.  67.  59.  21. 236. 254. 106.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.  83. 253. 209.  18.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.  22. 233. 255.  83.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0. 129. 254. 238.  44.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.  59. 249. 254.  62.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0. 133. 254. 187.   5.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   9. 205. 248.  58.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 126. 254. 182.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "   75. 251. 240.  57.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  19.\n",
      "  221. 254. 166.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   3. 203.\n",
      "  254. 219.  35.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  38. 254.\n",
      "  254.  77.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  31. 224. 254.\n",
      "  115.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 133. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.  61. 242. 254. 254.\n",
      "   52.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 254. 219.\n",
      "   40.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0. 121. 254. 207.  18.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.]]\n",
      "Min value: 0.0, Max value: 255.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, device, num_samples=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        # Move back to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        img = images[0]\n",
    "\n",
    "        img_np = img.squeeze().numpy()  # remove channel dimension\n",
    "\n",
    "        print(\"Pixel values of the first image:\")\n",
    "        print(img_np)\n",
    "\n",
    "        print(f\"Min value: {img_np.min()}, Max value: {img_np.max()}\")\n",
    "\n",
    "        # Create subplot\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Visualizing some predictions...\")\n",
    "visualize_predictions(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670e23b",
   "metadata": {},
   "source": [
    "## IPFE-enhanced CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7a0076c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.587281Z",
     "start_time": "2025-10-23T19:06:08.572405Z"
    }
   },
   "outputs": [],
   "source": [
    "# import ipfe functions\n",
    "from altered_ipfe import IPFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9809d3ef7052c",
   "metadata": {},
   "source": [
    "### current try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412789862c18ee8",
   "metadata": {},
   "source": [
    "In the First layer we applied 16 filters of size 3x3, with input channel 1 (grayscale), so the total number of parameters is 16 * 3 * 3 * 1 = 144.\n",
    "\n",
    "So With that each y is one filter flattened to a vector of length 9.\n",
    "For an input x of size 28x28, flattened to a vector of length 784, we can compute the inner product <x, y> for each filter y.\n",
    "\n",
    "Option 1 encrypt the entire input x of length 784, and padd the y to a length of 784 with zeros. -> This will be inefficient as the IPFE scheme will have to handle large vectors.\n",
    "\n",
    "Option 2 encrypt patches of x corresponding to the filter size (3x3 = 9 elements), and compute the inner product for each patch with the filter y. This requires sliding the filter over the input image and encrypting each patch separately.\n",
    "\n",
    "1. Step: Create patches of size 3x3 from the input image (28x28) -> This will create 28x28 patches (with padding). => 784 patches\n",
    "2. Step: Encrypt each patch separately.\n",
    "3. Step: Create 16 query vectors y (one for each filter), each of size 9 (3x3 flattened). => Create a Key for each vector\n",
    "4. Step: decrypt each inner product result to get the convolution output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e796720223be2333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.614332Z",
     "start_time": "2025-10-23T19:06:08.599694Z"
    }
   },
   "outputs": [],
   "source": [
    "class IPFECNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, prime=1000000007):\n",
    "        super(IPFECNN, self).__init__()\n",
    "        self.prime = prime\n",
    "        self.ipfe = IPFE(prime)\n",
    "        self.encryption_length = 5 # 3x3 filter size flattened\n",
    "\n",
    "        self.ipfe.setup(self.encryption_length)\n",
    "        print(\"IPFE setup done, with length:\", self.encryption_length)\n",
    "\n",
    "        # First convolutional block - this will be used with IPFE\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        #copy weights from the trained model\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        print(\"weights copied from trained model\")\n",
    "\n",
    "        self.weights = self.conv1.weight.data\n",
    "        self.y_array = torch.round(self.weights.view(self.weights.size(0), -1).squeeze(1).view(self.weights.size(0), -1) * 10000).long().tolist()\n",
    "        print(\"weights converted to y vectors\")\n",
    "        self.biases = self.conv1.bias\n",
    "        print(\"biases saved\")\n",
    "        self.sk_y_array = [self.ipfe.key_derive(y) for y in self.y_array]\n",
    "        print(\"sk_ys created\")\n",
    "\n",
    "    def first_conv_forward(self, x, encrypted=False):\n",
    "        if encrypted:\n",
    "            batch_size = x.shape[0]\n",
    "            H, W = x.shape[2], x.shape[3]\n",
    "\n",
    "            unfold = nn.Unfold(kernel_size=3, stride=1, padding=1)\n",
    "            patches = unfold(x)\n",
    "\n",
    "            num_patches = patches.shape[-1]\n",
    "            num_kernels = len(self.sk_y_array)\n",
    "\n",
    "            feature_maps_batch = []\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                patches_b = patches[b].T\n",
    "                decrypted_maps = torch.zeros(num_kernels, num_patches, device=x.device)\n",
    "\n",
    "                # Loop over patches and kernels\n",
    "                for p in range(num_patches):\n",
    "                    patch = patches_b[p]\n",
    "                    patch_int = [(int(val.item()) % (self.prime - 1)) for val in patch]\n",
    "                    encrypted = self.ipfe.encrypt(patch_int)\n",
    "\n",
    "                    for k in range(num_kernels):\n",
    "                        decrypted_scaled = self.ipfe.decrypt(encrypted, self.sk_y_array[k], self.y_array[k])\n",
    "\n",
    "                        decrypted = (decrypted_scaled / 10000) + self.biases[k].item() # bias needed W*X + B\n",
    "                        # (x1*y1*10000 + x2*y2*10000 + x3*y3*10000)/10000 = (x1*y1 + x2*y2 + x3*y3)\n",
    "\n",
    "                        decrypted_maps[k, p] = decrypted\n",
    "\n",
    "                # Reshape to feature map: (num_kernels, H, W)\n",
    "                feature_maps_b = decrypted_maps.view(num_kernels, H, W)\n",
    "                feature_maps_batch.append(feature_maps_b)\n",
    "\n",
    "            x_ipfe = torch.stack(feature_maps_batch, dim=0)  # (B, num_kernels, H, W)\n",
    "            # x_conv = self.conv1(x)\n",
    "            #\n",
    "            # b = 0\n",
    "            #\n",
    "            # diff = (x_conv - x_ipfe).abs()\n",
    "            # for c in range(num_kernels):\n",
    "            #     H, W= x_conv.shape[2], x_conv.shape[3]\n",
    "            #     h0, w0 = H // 2 - 2, W // 2 - 2  # top-left corner of 5x5 center\n",
    "            #     h1, w1 = h0 + 5, w0 + 5          # bottom-right corner\n",
    "            #\n",
    "            #     conv_center = x_conv[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "            #     ipfe_center = x_ipfe[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "            #     diff_center = diff[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "            #\n",
    "            #     print(f\"\\n=== Kernel {c}  Center 55 Region ===\")\n",
    "            #     print(\"Conv Output:\\n\", conv_center)\n",
    "            #     print(\"IPFE Output:\\n\", ipfe_center)\n",
    "            #     print(\"|Difference|:\\n\", diff_center)\n",
    "            #\n",
    "            # for c in range(num_kernels):\n",
    "            #     plt.figure(figsize=(9, 3))\n",
    "            #\n",
    "            #     # Conv Output\n",
    "            #     plt.subplot(1, 3, 1)\n",
    "            #     plt.imshow(x_conv[b, c].detach().cpu(), cmap=\"viridis\")\n",
    "            #     plt.title(f\"Conv Output (Kernel {c})\")\n",
    "            #     plt.axis(\"off\")\n",
    "            #\n",
    "            #     # IPFE Output\n",
    "            #     plt.subplot(1, 3, 2)\n",
    "            #     plt.imshow(x_ipfe[b, c].detach().cpu(), cmap=\"viridis\")\n",
    "            #     plt.title(f\"IPFE Output (Kernel {c})\")\n",
    "            #     plt.axis(\"off\")\n",
    "            #\n",
    "            #     # |Difference|\n",
    "            #     plt.subplot(1, 3, 3)\n",
    "            #     plt.imshow((x_conv[b, c] - x_ipfe[b, c]).abs().detach().cpu(), cmap=\"magma\")\n",
    "            #     plt.title(\"|Difference|\")\n",
    "            #     plt.axis(\"off\")\n",
    "            #\n",
    "            #     plt.tight_layout()\n",
    "            #     plt.show()\n",
    "\n",
    "            return x_ipfe\n",
    "\n",
    "        else:\n",
    "            return self.conv1(x)\n",
    "\n",
    "    def forward(self, x, encrypted=False):\n",
    "\n",
    "        x = self.first_conv_forward(x, encrypted)\n",
    "        x = self.pool1(F.relu(self.bn1(x)))\n",
    "\n",
    "        # use regular forward pass for remaining layers\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "333e22ae6922500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.639092Z",
     "start_time": "2025-10-23T19:06:08.629400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPFE setup done, with length: 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for IPFECNN:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 2, 2]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Initialize IPFE-enhanced CNN\u001b[39;00m\n\u001b[32m      2\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ipfe_model = \u001b[43mIPFECNN\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprime\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000000007\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPFE-CNN model created on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mIPFECNN.__init__\u001b[39m\u001b[34m(self, num_classes, prime)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.fc2 = nn.Linear(\u001b[32m128\u001b[39m, num_classes)\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m#copy weights from the trained model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mweights copied from trained model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28mself\u001b[39m.weights = \u001b[38;5;28mself\u001b[39m.conv1.weight.data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for IPFECNN:\n\tsize mismatch for conv1.weight: copying a param with shape torch.Size([16, 1, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 1, 2, 2])."
     ]
    }
   ],
   "source": [
    "# Initialize IPFE-enhanced CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ipfe_model = IPFECNN(num_classes=10, prime=1000000007).to(device)\n",
    "\n",
    "print(f\"IPFE-CNN model created on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd257fc471f05451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:16:14.476635Z",
     "start_time": "2025-10-23T19:09:02.705516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IPFE-CNN functionality...\n",
      "Labels of test samples: [7 2 1 0 4]\n",
      "Testing regular forward pass...\n",
      "Regular predictions: [7 2 1 0 4]\n",
      "Testing IPFE forward pass...\n",
      "IPFE predictions: [7 2 1 0 4]\n",
      "Prediction matches between regular and IPFE: 5/5\n"
     ]
    }
   ],
   "source": [
    "# Test IPFE functionality with a sample\n",
    "def test_ipfe_cnn(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "    # x is 784 big\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        print(f\"Labels of test samples: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "        # Test regular forward pass\n",
    "        print(\"Testing regular forward pass...\")\n",
    "        regular_outputs = model(images[:num_samples])\n",
    "        _, regular_predicted = regular_outputs.max(1)\n",
    "        print(\"Regular predictions:\", regular_predicted.cpu().numpy())\n",
    "\n",
    "        # Test IPFE forward pass\n",
    "        print(\"Testing IPFE forward pass...\")\n",
    "        try:\n",
    "            ipfe_outputs = model.forward(images[:num_samples], encrypted=True)\n",
    "            _, ipfe_predicted = ipfe_outputs.max(1)\n",
    "\n",
    "            print(f\"IPFE predictions: {ipfe_predicted.cpu().numpy()}\")\n",
    "\n",
    "            # Compare results\n",
    "            matches = (regular_predicted == ipfe_predicted).sum().item()\n",
    "            print(f\"Prediction matches between regular and IPFE: {matches}/{num_samples}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IPFE forward pass failed: {e}\")\n",
    "\n",
    "# Test the IPFE functionality\n",
    "print(\"Testing IPFE-CNN functionality...\")\n",
    "test_ipfe_cnn(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cc034993ee210",
   "metadata": {},
   "source": [
    "### Old try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b915ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define IPFE-enhanced CNN with functional encryption\n",
    "# class IPFECNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10, prime=104729):\n",
    "#         super(IPFECNN, self).__init__()\n",
    "#         self.prime = prime\n",
    "#         self.ipfe = IPFE(prime)\n",
    "#         self.input_size = None\n",
    "#         self.ipfe.setup(28*28)\n",
    "#         print(\"IPFE setup done\")\n",
    "#\n",
    "#\n",
    "#         # First convolutional block - this will be used with IPFE\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Second convolutional block\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Third convolutional block\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(64)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#\n",
    "#         # IPFE setup for first conv layer\n",
    "#         self.ipfe_setup_done = False\n",
    "#\n",
    "#\n",
    "#     def forward(self, x, y_vector=None):\n",
    "#\n",
    "#         # If y_vector is provided, use IPFE for first conv\n",
    "#         if y_vector is not None:\n",
    "#\n",
    "#             if not self.ipfe_setup_done:\n",
    "#                 self.setup_ipfe()\n",
    "#\n",
    "#             self.ipfe.key_derive(y_vector)\n",
    "#             print(f\"IPFE key derivation complete for provided query vector\")\n",
    "#\n",
    "#             # Encrypt the input x (flattened)\n",
    "#             x_flat = x.view(x.size(0), -1).cpu().numpy()\n",
    "#\n",
    "#             # For each sample in the batch, compute IPFE\n",
    "#             ipfe_results = []\n",
    "#             for x_i in x_flat:\n",
    "#                 # Convert input to integers\n",
    "#                 x_int = [int(val * 1000) % (self.prime - 1) for val in x_i]\n",
    "#\n",
    "#                 print(\"<x, y> (expected):\", sum((xi * yi) for xi, yi in zip(x_int, y_vector)) % (self.prime - 1))\n",
    "#\n",
    "#                 # Encrypt input\n",
    "#                 ct = self.ipfe.encrypt(x_int)\n",
    "#\n",
    "#                 # Decrypt to get inner product\n",
    "#                 try:\n",
    "#                     inner_product = self.ipfe.decrypt(ct)\n",
    "#\n",
    "#                     print(\"<x, y> (decrypted):\", inner_product)\n",
    "#\n",
    "#                     ipfe_results.append(inner_product)\n",
    "#                 except:\n",
    "#                     print(\"IPFE decryption failed\")\n",
    "#                     # Fallback to regular computation if IPFE fails\n",
    "#                     inner_product = sum(xi * yi for xi, yi in zip(x_int, y_vector)) % (self.prime - 1)\n",
    "#                     ipfe_results.append(inner_product)\n",
    "#\n",
    "#             # Convert IPFE results back to tensor\n",
    "#             ipfe_tensor = torch.tensor(ipfe_results, dtype=torch.float32).to(x.device)\n",
    "#\n",
    "#         # apply CNN normal\n",
    "#\n",
    "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "#\n",
    "#         # Continue with remaining layers\n",
    "#         x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "#         x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "#\n",
    "#         # Flatten and fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603797d67ee9a810",
   "metadata": {},
   "source": [
    "### run IPFE-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f05c5ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPFE setup done, with length: 9\n",
      "weights copied from trained model\n",
      "weights converted to y vectors\n",
      "biases saved\n",
      "sk_ys created\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for IPFECNN:\n\tMissing key(s) in state_dict: \"biases\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m ipfe_model = IPFECNN(num_classes=\u001b[32m10\u001b[39m, prime=\u001b[32m104729\u001b[39m).to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Copy weights from the trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mipfe_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m ipfe_model.setup_ipfe()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPFE-CNN model created on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for IPFECNN:\n\tMissing key(s) in state_dict: \"biases\". "
     ]
    }
   ],
   "source": [
    "# Initialize IPFE-enhanced CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ipfe_model = IPFECNN(num_classes=10, prime=104729).to(device)\n",
    "\n",
    "# Copy weights from the trained model\n",
    "ipfe_model.load_state_dict(model.state_dict())\n",
    "\n",
    "ipfe_model.setup_ipfe()\n",
    "\n",
    "print(f\"IPFE-CNN model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in ipfe_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61e2ae2d1d73ad79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IPFECNN' object has no attribute 'conv1_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mipfe_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1_length\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'IPFECNN' object has no attribute 'conv1_length'"
     ]
    }
   ],
   "source": [
    "print(ipfe_model.conv1_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IPFE functionality with a sample\n",
    "def test_ipfe_cnn(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "    # x is 784 big\n",
    "\n",
    "    # Create a sample query vector y (same length as flattened conv1 weights)\n",
    "    y_vector = [1] * model.conv1_length  # Simple query vector of all 1s\n",
    "    print(\"Using query vector y of length:\", len(y_vector))\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        print(f\"Labels of test samples: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "        # Test regular forward pass\n",
    "        print(\"Testing regular forward pass...\")\n",
    "        regular_outputs = model(images[:num_samples])\n",
    "        _, regular_predicted = regular_outputs.max(1)\n",
    "        print(\"Regular predictions:\", regular_predicted.cpu().numpy())\n",
    "\n",
    "        # Test IPFE forward pass\n",
    "        print(\"Testing IPFE forward pass...\")\n",
    "        try:\n",
    "            ipfe_outputs = model.forward_with_ipfe(images[:num_samples], y_vector)\n",
    "            _, ipfe_predicted = ipfe_outputs.max(1)\n",
    "\n",
    "            print(f\"Regular predictions: {regular_predicted.cpu().numpy()}\")\n",
    "            print(f\"IPFE predictions: {ipfe_predicted.cpu().numpy()}\")\n",
    "            print(f\"True labels: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "            # Compare results\n",
    "            matches = (regular_predicted == ipfe_predicted).sum().item()\n",
    "            print(f\"Prediction matches between regular and IPFE: {matches}/{num_samples}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IPFE forward pass failed: {e}\")\n",
    "\n",
    "# Test the IPFE functionality\n",
    "print(\"Testing IPFE-CNN functionality...\")\n",
    "test_ipfe_cnn(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate IPFE with different query vectors\n",
    "def demonstrate_ipfe_queries(model, test_loader, device):\n",
    "    \"\"\"Demonstrate IPFE with different query vectors\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a single test sample\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    single_image = images[0:1].to(device)  # Single sample\n",
    "    true_label = labels[0].item()\n",
    "    \n",
    "    print(f\"Testing with image of digit: {true_label}\")\n",
    "    \n",
    "    # Different query vectors to test\n",
    "    query_vectors = {\n",
    "        \"All ones\": [1] * model.conv1_length,\n",
    "        \"All zeros\": [0] * model.conv1_length,\n",
    "        \"Alternating\": [1 if i % 2 == 0 else -1 for i in range(model.conv1_length)],\n",
    "        \"Random\": [1, 0, 1, 0] * (model.conv1_length // 4) + [1] * (model.conv1_length % 4)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTesting different query vectors:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, y_vector in query_vectors.items():\n",
    "        try:\n",
    "            # Regular forward pass\n",
    "            regular_output = model(single_image)\n",
    "            regular_pred = regular_output.max(1)[1].item()\n",
    "            \n",
    "            # IPFE forward pass\n",
    "            ipfe_output = model.forward_with_ipfe(single_image, y_vector)\n",
    "            ipfe_pred = ipfe_output.max(1)[1].item()\n",
    "            \n",
    "            print(f\"{name:15} - Regular: {regular_pred}, IPFE: {ipfe_pred}, Match: {regular_pred == ipfe_pred}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} - IPFE failed: {str(e)[:50]}...\")\n",
    "\n",
    "# Demonstrate with different queries\n",
    "print(\"Demonstrating IPFE with different query vectors...\")\n",
    "demonstrate_ipfe_queries(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87803409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first CNN layer's filters (weights)\n",
    "def visualize_first_cnn_layer(model):\n",
    "    \"\"\"Visualize the first convolutional layer filters\"\"\"\n",
    "    # Find the first conv layer\n",
    "    first_conv = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            first_conv = m\n",
    "            break\n",
    "    if first_conv is None:\n",
    "        print(\"No Conv2d layer found in model.\")\n",
    "        return\n",
    "\n",
    "    weights = first_conv.weight.data.cpu()\n",
    "    num_filters = weights.shape[0]\n",
    "\n",
    "    # Calculate grid size\n",
    "    cols = 8\n",
    "    rows = (num_filters + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols*1.5, rows*1.5))\n",
    "    for i in range(num_filters):\n",
    "        ax = plt.subplot(rows, cols, i+1)\n",
    "        # For grayscale, show as [out_ch, in_ch, H, W]\n",
    "        w = weights[i, 0].numpy()\n",
    "        ax.imshow(w, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'F{i}')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"First Conv Layer Filters (IPFE-enhanced)\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing first CNN layer filters...\")\n",
    "visualize_first_cnn_layer(ipfe_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaa144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc48825",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
