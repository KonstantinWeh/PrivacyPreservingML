{
 "cells": [
  {
<<<<<<< HEAD:cnn_ipfe.ipynb
   "cell_type": "markdown",
   "id": "c2760c72956b3d3",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
=======
>>>>>>> main:01_modified_cnn.ipynb
   "cell_type": "code",
   "execution_count": 10,
   "id": "c29d01d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:09:59.898469Z",
     "start_time": "2025-11-13T13:09:41.207682Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
<<<<<<< HEAD:cnn_ipfe.ipynb
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81bbf67e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.521513Z",
     "start_time": "2025-10-23T19:04:40.976278Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n"
   ]
=======
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:13:19.930281Z",
     "start_time": "2025-11-13T13:13:19.922820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "version = 1\n",
    "save_dir = \"models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model_path = os.path.join(save_dir, f\"cnn_model_{version}.pth\")"
   ],
   "id": "a0ed36110404ca1f",
   "outputs": [],
   "execution_count": 13
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD:cnn_ipfe.ipynb
   "id": "ca69fe0bf0214053",
   "metadata": {},
   "source": [
    "## Trained CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec44f3165f628cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:04:46.589824Z",
     "start_time": "2025-10-23T19:04:46.539338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda pic: torch.tensor(np.array(pic), dtype=torch.float32).unsqueeze(0))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
=======
   "source": "## Define Model",
   "id": "8905c1b062a665ed"
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f8ff355",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:09:59.958508Z",
     "start_time": "2025-11-13T13:09:59.946406Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a lightweight CNN architecture\n",
    "class LightweightCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(LightweightCNN, self).__init__()\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1) # stride = 2, padding = 0\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)  # 64 * 1 * 1, 128\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
<<<<<<< HEAD:cnn_ipfe.ipynb
   ]
=======
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initialize Model",
   "id": "fd42fea09d235de0"
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7693a69ca22d4ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:10:00.579275Z",
     "start_time": "2025-11-13T13:10:00.574203Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.01)  # smaller std for large inputs\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
<<<<<<< HEAD:cnn_ipfe.ipynb
   ]
=======
   ],
   "id": "6d473294eefc9206",
   "outputs": [],
   "execution_count": 4
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb3f84ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:10:00.625164Z",
     "start_time": "2025-11-13T13:10:00.599224Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cpu\n",
      "Total parameters: 98,666\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LightweightCNN(num_classes=10).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "print(f\"Model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n"
<<<<<<< HEAD:cnn_ipfe.ipynb
   ]
=======
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created on device: cpu\n",
      "Total parameters: 98,666\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Load Data",
   "id": "c807c56097fa90ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:10:00.940848Z",
     "start_time": "2025-11-13T13:10:00.743227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda pic: torch.tensor(np.array(pic), dtype=torch.float32).unsqueeze(0))\n",
    "])\n",
    "\n",
    "# Load training and test datasets\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ],
   "id": "ad87040d9fac7caa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n"
     ]
    }
   ],
   "execution_count": 6
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train model",
   "id": "147d04b813057a72"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe24be06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:11:34.838647Z",
     "start_time": "2025-11-13T13:10:00.960232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Batch 0/938, Loss: 2.3124, Accuracy: 9.38%\n",
      "Epoch 1/3, Batch 200/938, Loss: 0.1820, Accuracy: 81.61%\n",
      "Epoch 1/3, Batch 400/938, Loss: 0.1468, Accuracy: 88.47%\n",
      "Epoch 1/3, Batch 600/938, Loss: 0.1162, Accuracy: 91.08%\n",
      "Epoch 1/3, Batch 800/938, Loss: 0.0418, Accuracy: 92.60%\n",
      "Epoch 1 completed - Loss: 0.2340, Accuracy: 93.32%\n",
      "--------------------------------------------------\n",
      "Epoch 2/3, Batch 0/938, Loss: 0.0872, Accuracy: 95.31%\n",
      "Epoch 2/3, Batch 200/938, Loss: 0.0568, Accuracy: 97.79%\n",
      "Epoch 2/3, Batch 400/938, Loss: 0.1235, Accuracy: 97.80%\n",
      "Epoch 2/3, Batch 600/938, Loss: 0.0150, Accuracy: 97.87%\n",
      "Epoch 2/3, Batch 800/938, Loss: 0.1051, Accuracy: 97.93%\n",
      "Epoch 2 completed - Loss: 0.0688, Accuracy: 97.97%\n",
      "--------------------------------------------------\n",
      "Epoch 3/3, Batch 0/938, Loss: 0.0449, Accuracy: 96.88%\n",
      "Epoch 3/3, Batch 200/938, Loss: 0.0278, Accuracy: 98.34%\n",
      "Epoch 3/3, Batch 400/938, Loss: 0.0359, Accuracy: 98.34%\n",
      "Epoch 3/3, Batch 600/938, Loss: 0.0608, Accuracy: 98.42%\n",
      "Epoch 3/3, Batch 800/938, Loss: 0.0675, Accuracy: 98.47%\n",
      "Epoch 3 completed - Loss: 0.0513, Accuracy: 98.48%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            if batch_idx % 200 == 0:\n",
    "                print(f'Epoch {epoch + 1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {100. * correct / total:.2f}%')\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        print(f'Epoch {epoch + 1} completed - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "train_model(model, train_loader, criterion, optimizer, device, epochs=3)\n"
<<<<<<< HEAD:cnn_ipfe.ipynb
   ]
=======
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/3, Batch 0/938, Loss: 2.4346, Accuracy: 12.50%\n",
      "Epoch 1/3, Batch 200/938, Loss: 0.1991, Accuracy: 80.33%\n",
      "Epoch 1/3, Batch 400/938, Loss: 0.1027, Accuracy: 87.86%\n",
      "Epoch 1/3, Batch 600/938, Loss: 0.1891, Accuracy: 90.72%\n",
      "Epoch 1/3, Batch 800/938, Loss: 0.0922, Accuracy: 92.26%\n",
      "Epoch 1 completed - Loss: 0.2473, Accuracy: 92.94%\n",
      "--------------------------------------------------\n",
      "Epoch 2/3, Batch 0/938, Loss: 0.1198, Accuracy: 93.75%\n",
      "Epoch 2/3, Batch 200/938, Loss: 0.0818, Accuracy: 97.74%\n",
      "Epoch 2/3, Batch 400/938, Loss: 0.0388, Accuracy: 97.78%\n",
      "Epoch 2/3, Batch 600/938, Loss: 0.1407, Accuracy: 97.74%\n",
      "Epoch 2/3, Batch 800/938, Loss: 0.1343, Accuracy: 97.79%\n",
      "Epoch 2 completed - Loss: 0.0741, Accuracy: 97.83%\n",
      "--------------------------------------------------\n",
      "Epoch 3/3, Batch 0/938, Loss: 0.0260, Accuracy: 100.00%\n",
      "Epoch 3/3, Batch 200/938, Loss: 0.0213, Accuracy: 98.31%\n",
      "Epoch 3/3, Batch 400/938, Loss: 0.0127, Accuracy: 98.38%\n",
      "Epoch 3/3, Batch 600/938, Loss: 0.0174, Accuracy: 98.40%\n",
      "Epoch 3/3, Batch 800/938, Loss: 0.0319, Accuracy: 98.41%\n",
      "Epoch 3 completed - Loss: 0.0550, Accuracy: 98.43%\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 7
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test Model",
   "id": "216ef7fa79a68e6e"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ec1ed97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:11:37.240154Z",
     "start_time": "2025-11-13T13:11:34.898470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Loss: 0.0364\n",
      "Test Accuracy: 98.74%\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / total\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_accuracy = test_model(model, test_loader, device)\n"
<<<<<<< HEAD:cnn_ipfe.ipynb
   ]
=======
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n",
      "Test Loss: 0.0347\n",
      "Test Accuracy: 98.72%\n"
     ]
    }
   ],
   "execution_count": 8
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66e75db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:11:37.952624Z",
     "start_time": "2025-11-13T13:11:37.269032Z"
    }
   },
<<<<<<< HEAD:cnn_ipfe.ipynb
=======
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, device, num_samples=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        # Move back to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        img = images[0]\n",
    "\n",
    "        img_np = img.squeeze().numpy()  # remove channel dimension\n",
    "\n",
    "        # Create subplot\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Visualizing some predictions...\")\n",
    "visualize_predictions(model, test_loader, device)\n"
   ],
>>>>>>> main:01_modified_cnn.ipynb
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizing some predictions...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some predictions\n",
    "def visualize_predictions(model, test_loader, device, num_samples=8):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Get predictions\n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "\n",
    "        # Move back to CPU for visualization\n",
    "        images = images.cpu()\n",
    "        labels = labels.cpu()\n",
    "        predicted = predicted.cpu()\n",
    "\n",
    "        img = images[0]\n",
    "\n",
    "        img_np = img.squeeze().numpy()  # remove channel dimension\n",
    "\n",
    "        print(\"Pixel values of the first image:\")\n",
    "        print(img_np)\n",
    "\n",
    "        print(f\"Min value: {img_np.min()}, Max value: {img_np.max()}\")\n",
    "\n",
    "        # Create subplot\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "        axes = axes.ravel()\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            axes[i].imshow(images[i].squeeze(), cmap='gray')\n",
    "            axes[i].set_title(f'True: {labels[i].item()}, Pred: {predicted[i].item()}')\n",
    "            axes[i].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Visualize predictions\n",
    "print(\"Visualizing some predictions...\")\n",
    "visualize_predictions(model, test_loader, device)\n"
   ]
  },
  {
<<<<<<< HEAD:cnn_ipfe.ipynb
   "cell_type": "markdown",
   "id": "a670e23b",
   "metadata": {},
   "source": [
    "## IPFE-enhanced CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a0076c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.587281Z",
     "start_time": "2025-10-23T19:06:08.572405Z"
    }
   },
   "outputs": [],
   "source": [
    "# import ipfe functions\n",
    "from altered_ipfe import IPFE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e9809d3ef7052c",
   "metadata": {},
   "source": [
    "### current try"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e412789862c18ee8",
   "metadata": {},
   "source": [
    "In the First layer we applied 16 filters of size 3x3, with input channel 1 (grayscale), so the total number of parameters is 16 * 3 * 3 * 1 = 144.\n",
    "\n",
    "So With that each y is one filter flattened to a vector of length 9.\n",
    "For an input x of size 28x28, flattened to a vector of length 784, we can compute the inner product <x, y> for each filter y.\n",
    "\n",
    "Option 1 encrypt the entire input x of length 784, and padd the y to a length of 784 with zeros. -> This will be inefficient as the IPFE scheme will have to handle large vectors.\n",
    "\n",
    "Option 2 encrypt patches of x corresponding to the filter size (3x3 = 9 elements), and compute the inner product for each patch with the filter y. This requires sliding the filter over the input image and encrypting each patch separately.\n",
    "\n",
    "1. Step: Create patches of size 3x3 from the input image (28x28) -> This will create 28x28 patches (with padding). => 784 patches\n",
    "2. Step: Encrypt each patch separately.\n",
    "3. Step: Create 16 query vectors y (one for each filter), each of size 9 (3x3 flattened). => Create a Key for each vector\n",
    "4. Step: decrypt each inner product result to get the convolution output."
   ]
=======
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Save Model",
   "id": "157c988d66f5f588"
>>>>>>> main:01_modified_cnn.ipynb
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e796720223be2333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-13T13:13:24.801020Z",
     "start_time": "2025-11-13T13:13:24.790993Z"
    }
   },
   "outputs": [],
   "source": [
<<<<<<< HEAD:cnn_ipfe.ipynb
    "class IPFECNN(nn.Module):\n",
    "    def __init__(self, num_classes=10, prime=1000000007):\n",
    "        super(IPFECNN, self).__init__()\n",
    "        self.prime = prime\n",
    "        self.ipfe = IPFE(prime)\n",
    "        self.encryption_length = 9 # 3x3 filter size flattened\n",
    "\n",
    "        self.ipfe.setup(self.encryption_length)\n",
    "        print(\"IPFE setup done, with length:\", self.encryption_length)\n",
    "\n",
    "        # First convolutional block - this will be used with IPFE\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Third convolutional block\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "        #copy weights from the trained model\n",
    "        self.load_state_dict(model.state_dict())\n",
    "        print(\"weights copied from trained model\")\n",
    "\n",
    "        self.weights = self.conv1.weight.data\n",
    "        self.y_array = torch.round(self.weights.view(self.weights.size(0), -1).squeeze(1).view(self.weights.size(0), -1) * 10000).long().tolist()\n",
    "        print(\"weights converted to y vectors\")\n",
    "        self.biases = self.conv1.bias\n",
    "        print(\"biases saved\")\n",
    "        self.sk_y_array = [self.ipfe.key_derive(y) for y in self.y_array]\n",
    "        print(\"sk_ys created\", self.sk_y_array)\n",
    "\n",
    "    def first_conv_forward(self, x, encrypted=False):\n",
    "        if encrypted:\n",
    "            batch_size = x.shape[0]\n",
    "            H, W = x.shape[2], x.shape[3]\n",
    "\n",
    "            unfold = nn.Unfold(kernel_size=3, stride=1, padding=1)\n",
    "            patches = unfold(x)\n",
    "\n",
    "            num_patches = patches.shape[-1]\n",
    "            print(\"num_patches\", num_patches)\n",
    "            num_kernels = len(self.sk_y_array)\n",
    "\n",
    "            feature_maps_batch = []\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                patches_b = patches[b].T\n",
    "                decrypted_maps = torch.zeros(num_kernels, num_patches, device=x.device)\n",
    "\n",
    "                # Loop over patches and kernels\n",
    "                for p in range(num_patches):\n",
    "                    patch = patches_b[p]\n",
    "                    # view_patch = patch.detach().cpu()\n",
    "                    # plt.imshow(view_patch, cmap='viridis')\n",
    "                    # plt.show()\n",
    "                    # plt.close()\n",
    "                    patch_int = [(int(val.item()) % (self.prime - 1)) for val in patch]\n",
    "                    encrypted = self.ipfe.encrypt(patch_int)\n",
    "\n",
    "                    for k in range(num_kernels):\n",
    "                        decrypted_scaled = self.ipfe.decrypt(encrypted, self.sk_y_array[k], self.y_array[k])\n",
    "\n",
    "                        decrypted = (decrypted_scaled / 10000) + self.biases[k].item() # bias needed W*X + B\n",
    "                        # (x1*y1*10000 + x2*y2*10000 + x3*y3*10000)/10000 = (x1*y1 + x2*y2 + x3*y3)\n",
    "\n",
    "                        decrypted_maps[k, p] = decrypted\n",
    "\n",
    "                # Reshape to feature map: (num_kernels, H, W)\n",
    "                feature_maps_b = decrypted_maps.view(num_kernels, H, W)\n",
    "                feature_maps_batch.append(feature_maps_b)\n",
    "\n",
    "            x_ipfe = torch.stack(feature_maps_batch, dim=0)  # (B, num_kernels, H, W)\n",
    "                \n",
    "            x_conv = self.conv1(x)\n",
    "            \n",
    "            b = 0\n",
    "            \n",
    "            diff = (x_conv - x_ipfe).abs()\n",
    "\n",
    "            plt.imshow(diff[0, 0])\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            for c in range(num_kernels):\n",
    "                H, W= x_conv.shape[2], x_conv.shape[3]\n",
    "                h0, w0 = H // 2 - 2, W // 2 - 2  # top-left corner of 5x5 center\n",
    "                h1, w1 = h0 + 5, w0 + 5          # bottom-right corner\n",
    "            \n",
    "                conv_center = x_conv[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "                ipfe_center = x_ipfe[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "                diff_center = diff[b, c, h0:h1, w0:w1].detach().cpu()\n",
    "            \n",
    "                print(f\"\\n=== Kernel {c}  Center 55 Region ===\")\n",
    "                print(\"Conv Output:\\n\", conv_center)\n",
    "                print(\"IPFE Output:\\n\", ipfe_center)\n",
    "                print(\"|Difference|:\\n\", diff_center)\n",
    "\n",
    "            return x_ipfe\n",
    "\n",
    "        else:\n",
    "            return self.conv1(x)\n",
    "\n",
    "    def forward(self, x, encrypted=False):\n",
    "\n",
    "        x = self.first_conv_forward(x, encrypted)\n",
    "        x = self.pool1(F.relu(self.bn1(x)))\n",
    "\n",
    "        # use regular forward pass for remaining layers\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "\n",
    "        # Flatten and fully connected layers\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "333e22ae6922500",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:06:08.639092Z",
     "start_time": "2025-10-23T19:06:08.629400Z"
    }
   },
=======
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ],
   "id": "993f34461fc4131d",
>>>>>>> main:01_modified_cnn.ipynb
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD:cnn_ipfe.ipynb
      "IPFE setup done, with length: 9\n",
      "weights copied from trained model\n",
      "weights converted to y vectors\n",
      "biases saved\n",
      "sk_ys created [857271409, 937936417, 874272113, 235753203, 66559675, 300758675, 342935151, 425219013, 932586012, 446574807, 302710867, 899024170, 735561186, 200352738, 265239914, 786444431]\n",
      "IPFE-CNN model created on device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize IPFE-enhanced CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ipfe_model = IPFECNN(num_classes=10, prime=1000000007).to(device)\n",
    "\n",
    "print(f\"IPFE-CNN model created on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd257fc471f05451",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-23T19:16:14.476635Z",
     "start_time": "2025-10-23T19:09:02.705516Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing IPFE-CNN functionality...\n",
      "Labels of test samples: [7]\n",
      "Testing regular forward pass...\n",
      "Regular predictions: [7]\n",
      "Testing IPFE forward pass...\n",
      "num_patches 784\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Kernel 0  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[  0.1367,   0.1367,   0.1367,   0.1367,   3.0944],\n",
      "        [  0.1367,   0.1367,   0.1367,   2.1684,  -7.2451],\n",
      "        [  0.1367,   0.1367,   0.1367,   0.7355, -13.9678],\n",
      "        [  0.1367,   0.1367,   0.4466,   7.9214, -28.3199],\n",
      "        [  0.1367,   0.1367,   3.8683,  -4.5132, -41.1130]])\n",
      "IPFE Output:\n",
      " tensor([[  0.1367,   0.1367,   0.1367,   0.1367,   3.0893],\n",
      "        [  0.1367,   0.1367,   0.1367,   2.1663,  -7.2555],\n",
      "        [  0.1367,   0.1367,   0.1367,   0.7294, -13.9775],\n",
      "        [  0.1367,   0.1367,   0.4463,   7.9113, -28.3263],\n",
      "        [  0.1367,   0.1367,   3.8636,  -4.5236, -41.1177]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0051],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0021, 0.0104],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0061, 0.0097],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0101, 0.0064],\n",
      "        [0.0000, 0.0000, 0.0047, 0.0104, 0.0047]])\n",
      "\n",
      "=== Kernel 1  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ 0.2963,  0.2963,  0.2963,  0.2963, -0.6455],\n",
      "        [ 0.2963,  0.2963,  0.2963,  0.5606, 16.9726],\n",
      "        [ 0.2963,  0.2963,  0.2963, -3.1832, 39.8416],\n",
      "        [ 0.2963,  0.2963,  0.3366,  6.0980, 70.0625],\n",
      "        [ 0.2963,  0.2963,  0.2390, 53.3318, 84.3215]])\n",
      "IPFE Output:\n",
      " tensor([[ 0.2963,  0.2963,  0.2963,  0.2963, -0.6434],\n",
      "        [ 0.2963,  0.2963,  0.2963,  0.5618, 16.9750],\n",
      "        [ 0.2963,  0.2963,  0.2963, -3.1821, 39.8456],\n",
      "        [ 0.2963,  0.2963,  0.3368,  6.1002, 70.0732],\n",
      "        [ 0.2963,  0.2963,  0.2414, 53.3357, 84.3365]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0021],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0013, 0.0024],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0012, 0.0041],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0022, 0.0107],\n",
      "        [0.0000, 0.0000, 0.0025, 0.0039, 0.0151]])\n",
      "\n",
      "=== Kernel 2  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[  0.2279,   0.2279,   0.2279,   0.2279, -37.5234],\n",
      "        [  0.2279,   0.2279,   0.2279, -16.9223, -78.3537],\n",
      "        [  0.2279,   0.2279,   0.2279, -39.1120, -55.9651],\n",
      "        [  0.2279,   0.2279,  -2.3882, -42.8560, -32.9227],\n",
      "        [  0.2279,   0.2279, -36.5017, -56.3766, -39.3229]])\n",
      "IPFE Output:\n",
      " tensor([[  0.2279,   0.2279,   0.2279,   0.2279, -37.5254],\n",
      "        [  0.2279,   0.2279,   0.2279, -16.9234, -78.3541],\n",
      "        [  0.2279,   0.2279,   0.2279, -39.1137, -55.9604],\n",
      "        [  0.2279,   0.2279,  -2.3884, -42.8566, -32.9145],\n",
      "        [  0.2279,   0.2279, -36.5038, -56.3718, -39.3188]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0020],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0010, 0.0004],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0016, 0.0047],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0005, 0.0082],\n",
      "        [0.0000, 0.0000, 0.0021, 0.0049, 0.0042]])\n",
      "\n",
      "=== Kernel 3  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ -0.2681,  -0.2681,  -0.2681,  -0.2681, -26.3284],\n",
      "        [ -0.2681,  -0.2681,  -0.2681, -14.6425, -21.3489],\n",
      "        [ -0.2681,  -0.2681,  -0.2681, -18.2741,  33.7653],\n",
      "        [ -0.2681,  -0.2681,  -2.4608,  -9.2341,  80.7184],\n",
      "        [ -0.2681,  -0.2681, -28.7698,  17.4372, 156.4097]])\n",
      "IPFE Output:\n",
      " tensor([[ -0.2681,  -0.2681,  -0.2681,  -0.2681, -26.3245],\n",
      "        [ -0.2681,  -0.2681,  -0.2681, -14.6405, -21.3450],\n",
      "        [ -0.2681,  -0.2681,  -0.2681, -18.2709,  33.7671],\n",
      "        [ -0.2681,  -0.2681,  -2.4605,  -9.2286,  80.7208],\n",
      "        [ -0.2681,  -0.2681, -28.7657,  17.4406, 156.4052]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0039],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0020, 0.0040],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0032, 0.0019],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0055, 0.0025],\n",
      "        [0.0000, 0.0000, 0.0041, 0.0034, 0.0045]])\n",
      "\n",
      "=== Kernel 4  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[-0.1026, -0.1026, -0.1026, -0.1026, 11.9142],\n",
      "        [-0.1026, -0.1026, -0.1026,  5.3566, 34.7670],\n",
      "        [-0.1026, -0.1026, -0.1026, 12.4195, 34.8657],\n",
      "        [-0.1026, -0.1026,  0.7301,  4.4761, 15.7995],\n",
      "        [-0.1026, -0.1026, 11.5891, 24.0513, 12.1340]])\n",
      "IPFE Output:\n",
      " tensor([[-0.1026, -0.1026, -0.1026, -0.1026, 11.9113],\n",
      "        [-0.1026, -0.1026, -0.1026,  5.3549, 34.7644],\n",
      "        [-0.1026, -0.1026, -0.1026, 12.4182, 34.8679],\n",
      "        [-0.1026, -0.1026,  0.7299,  4.4747, 15.8030],\n",
      "        [-0.1026, -0.1026, 11.5857, 24.0495, 12.1413]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0029],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0018, 0.0026],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0013, 0.0022],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0014, 0.0035],\n",
      "        [0.0000, 0.0000, 0.0034, 0.0019, 0.0073]])\n",
      "\n",
      "=== Kernel 5  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ 9.0056e-04,  9.0056e-04,  9.0056e-04,  9.0056e-04,  1.4952e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  9.0056e-04,  9.6430e+00, -8.4297e+00],\n",
      "        [ 9.0056e-04,  9.0056e-04,  9.0056e-04,  5.2958e+00, -4.4969e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  1.4717e+00,  7.1632e+00, -6.0117e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  1.8085e+01, -2.4922e+01, -1.1711e+02]])\n",
      "IPFE Output:\n",
      " tensor([[ 9.0056e-04,  9.0056e-04,  9.0056e-04,  9.0056e-04,  1.4948e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  9.0056e-04,  9.6415e+00, -8.4445e+00],\n",
      "        [ 9.0056e-04,  9.0056e-04,  9.0056e-04,  5.2898e+00, -4.4992e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  1.4715e+00,  7.1517e+00, -6.0143e+01],\n",
      "        [ 9.0056e-04,  9.0056e-04,  1.8081e+01, -2.4943e+01, -1.1715e+02]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0043],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0015, 0.0148],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0060, 0.0221],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0115, 0.0256],\n",
      "        [0.0000, 0.0000, 0.0036, 0.0209, 0.0309]])\n",
      "\n",
      "=== Kernel 6  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[   0.2088,    0.2088,    0.2088,    0.2088,   31.8545],\n",
      "        [   0.2088,    0.2088,    0.2088,   16.9184,   28.3725],\n",
      "        [   0.2088,    0.2088,    0.2088,   24.7651,  -14.4803],\n",
      "        [   0.2088,    0.2088,    2.7578,   39.5859,  -46.9014],\n",
      "        [   0.2088,    0.2088,   33.8937,   14.5035, -114.8705]])\n",
      "IPFE Output:\n",
      " tensor([[   0.2088,    0.2088,    0.2088,    0.2088,   31.8532],\n",
      "        [   0.2088,    0.2088,    0.2088,   16.9176,   28.3706],\n",
      "        [   0.2088,    0.2088,    0.2088,   24.7646,  -14.4830],\n",
      "        [   0.2088,    0.2088,    2.7576,   39.5832,  -46.9044],\n",
      "        [   0.2088,    0.2088,   33.8923,   14.4977, -114.8617]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0012],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0008, 0.0020],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0005, 0.0026],\n",
      "        [0.0000, 0.0000, 0.0001, 0.0026, 0.0029],\n",
      "        [0.0000, 0.0000, 0.0015, 0.0058, 0.0088]])\n",
      "\n",
      "=== Kernel 7  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[  0.1984,   0.1984,   0.1984,   0.1984,  16.9669],\n",
      "        [  0.1984,   0.1984,   0.1984,   8.4396,  30.3641],\n",
      "        [  0.1984,   0.1984,   0.1984,  15.4230,  15.1226],\n",
      "        [  0.1984,   0.1984,   1.4555,  17.0804,   1.2431],\n",
      "        [  0.1984,   0.1984,  17.2867,  21.7962, -31.4630]])\n",
      "IPFE Output:\n",
      " tensor([[  0.1984,   0.1984,   0.1984,   0.1984,  16.9701],\n",
      "        [  0.1984,   0.1984,   0.1984,   8.4407,  30.3720],\n",
      "        [  0.1984,   0.1984,   0.1984,  15.4273,  15.1324],\n",
      "        [  0.1984,   0.1984,   1.4557,  17.0894,   1.2531],\n",
      "        [  0.1984,   0.1984,  17.2894,  21.8071, -31.4516]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0032],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0012, 0.0079],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0043, 0.0098],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0090, 0.0100],\n",
      "        [0.0000, 0.0000, 0.0027, 0.0109, 0.0114]])\n",
      "\n",
      "=== Kernel 8  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ -0.2494,  -0.2494,  -0.2494,  -0.2494, -28.6670],\n",
      "        [ -0.2494,  -0.2494,  -0.2494, -16.2380, -33.1209],\n",
      "        [ -0.2494,  -0.2494,  -0.2494, -18.7506, -22.4837],\n",
      "        [ -0.2494,  -0.2494,  -2.6883, -21.6836, -47.8613],\n",
      "        [ -0.2494,  -0.2494, -31.7189, -25.4731, -39.2753]])\n",
      "IPFE Output:\n",
      " tensor([[ -0.2494,  -0.2494,  -0.2494,  -0.2494, -28.6678],\n",
      "        [ -0.2494,  -0.2494,  -0.2494, -16.2384, -33.1241],\n",
      "        [ -0.2494,  -0.2494,  -0.2494, -18.7517, -22.4930],\n",
      "        [ -0.2494,  -0.2494,  -2.6884, -21.6873, -47.8770],\n",
      "        [ -0.2494,  -0.2494, -31.7197, -25.4810, -39.2915]])\n",
      "|Difference|:\n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.0299e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 3.1471e-04, 3.1929e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0262e-03, 9.3021e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 4.7684e-05, 3.6774e-03, 1.5671e-02],\n",
      "        [0.0000e+00, 0.0000e+00, 7.1526e-04, 7.8278e-03, 1.6205e-02]])\n",
      "\n",
      "=== Kernel 9  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[4.2342e-03, 4.2342e-03, 4.2342e-03, 4.2342e-03, 1.2717e+01],\n",
      "        [4.2342e-03, 4.2342e-03, 4.2342e-03, 3.4688e+00, 6.5256e+01],\n",
      "        [4.2342e-03, 4.2342e-03, 4.2342e-03, 2.1592e+01, 1.2078e+02],\n",
      "        [4.2342e-03, 4.2342e-03, 5.3273e-01, 5.1410e+01, 1.3395e+02],\n",
      "        [4.2342e-03, 4.2342e-03, 9.5048e+00, 1.1369e+02, 8.9899e+01]])\n",
      "IPFE Output:\n",
      " tensor([[4.2342e-03, 4.2342e-03, 4.2342e-03, 4.2342e-03, 1.2714e+01],\n",
      "        [4.2342e-03, 4.2342e-03, 4.2342e-03, 3.4675e+00, 6.5245e+01],\n",
      "        [4.2342e-03, 4.2342e-03, 4.2342e-03, 2.1588e+01, 1.2076e+02],\n",
      "        [4.2342e-03, 4.2342e-03, 5.3253e-01, 5.1401e+01, 1.3394e+02],\n",
      "        [4.2342e-03, 4.2342e-03, 9.5019e+00, 1.1368e+02, 8.9886e+01]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0032],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0013, 0.0101],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0040, 0.0151],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0089, 0.0187],\n",
      "        [0.0000, 0.0000, 0.0029, 0.0162, 0.0128]])\n",
      "\n",
      "=== Kernel 10  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ 4.0936e-02,  4.0936e-02,  4.0936e-02,  4.0936e-02,  3.1570e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  4.0936e-02,  1.2977e+01,  5.9220e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  4.0936e-02,  3.7904e+01,  5.8045e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  2.0142e+00,  5.5116e+01,  1.9949e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  2.8994e+01,  4.5167e+01, -4.5289e+01]])\n",
      "IPFE Output:\n",
      " tensor([[ 4.0936e-02,  4.0936e-02,  4.0936e-02,  4.0936e-02,  3.1576e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  4.0936e-02,  1.2980e+01,  5.9236e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  4.0936e-02,  3.7910e+01,  5.8067e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  2.0146e+00,  5.5129e+01,  1.9973e+01],\n",
      "        [ 4.0936e-02,  4.0936e-02,  2.9000e+01,  4.5190e+01, -4.5268e+01]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0061],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0027, 0.0160],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0067, 0.0224],\n",
      "        [0.0000, 0.0000, 0.0004, 0.0129, 0.0247],\n",
      "        [0.0000, 0.0000, 0.0058, 0.0230, 0.0207]])\n",
      "\n",
      "=== Kernel 11  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ -0.1598,  -0.1598,  -0.1598,  -0.1598, -27.9327],\n",
      "        [ -0.1598,  -0.1598,  -0.1598, -13.7365, -57.1678],\n",
      "        [ -0.1598,  -0.1598,  -0.1598, -25.6379, -52.0778],\n",
      "        [ -0.1598,  -0.1598,  -2.2308, -20.8126, -35.5963],\n",
      "        [ -0.1598,  -0.1598, -28.3720, -40.5781, -18.6872]])\n",
      "IPFE Output:\n",
      " tensor([[ -0.1598,  -0.1598,  -0.1598,  -0.1598, -27.9309],\n",
      "        [ -0.1598,  -0.1598,  -0.1598, -13.7357, -57.1645],\n",
      "        [ -0.1598,  -0.1598,  -0.1598, -25.6360, -52.0754],\n",
      "        [ -0.1598,  -0.1598,  -2.2307, -20.8119, -35.5952],\n",
      "        [ -0.1598,  -0.1598, -28.3703, -40.5783, -18.6794]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0018],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0008, 0.0033],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0020, 0.0024],\n",
      "        [0.0000, 0.0000, 0.0001, 0.0007, 0.0012],\n",
      "        [0.0000, 0.0000, 0.0017, 0.0002, 0.0079]])\n",
      "\n",
      "=== Kernel 12  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[-8.6876e-02, -8.6876e-02, -8.6876e-02, -8.6876e-02, -9.2766e+00],\n",
      "        [-8.6876e-02, -8.6876e-02, -8.6876e-02, -6.0353e+00,  1.7097e+01],\n",
      "        [-8.6876e-02, -8.6876e-02, -8.6876e-02, -3.2621e+00,  8.4793e+01],\n",
      "        [-8.6876e-02, -8.6876e-02, -9.9426e-01,  2.3838e+01,  1.1915e+02],\n",
      "        [-8.6876e-02, -8.6876e-02, -1.1229e+01,  8.0359e+01,  1.2185e+02]])\n",
      "IPFE Output:\n",
      " tensor([[-8.6876e-02, -8.6876e-02, -8.6876e-02, -8.6876e-02, -9.2731e+00],\n",
      "        [-8.6876e-02, -8.6876e-02, -8.6876e-02, -6.0341e+00,  1.7106e+01],\n",
      "        [-8.6876e-02, -8.6876e-02, -8.6876e-02, -3.2568e+00,  8.4803e+01],\n",
      "        [-8.6876e-02, -8.6876e-02, -9.9408e-01,  2.3847e+01,  1.1915e+02],\n",
      "        [-8.6876e-02, -8.6876e-02, -1.1226e+01,  8.0368e+01,  1.2185e+02]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0036],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0012, 0.0090],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0053, 0.0108],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0085, 0.0059],\n",
      "        [0.0000, 0.0000, 0.0029, 0.0086, 0.0020]])\n",
      "\n",
      "=== Kernel 13  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[  0.3122,   0.3122,   0.3122,   0.3122, -38.1023],\n",
      "        [  0.3122,   0.3122,   0.3122, -15.0593, -86.7874],\n",
      "        [  0.3122,   0.3122,   0.3122, -47.2268, -97.9503],\n",
      "        [  0.3122,   0.3122,  -2.0326, -71.8810, -93.4805],\n",
      "        [  0.3122,   0.3122, -34.4810, -84.1438, -93.7550]])\n",
      "IPFE Output:\n",
      " tensor([[  0.3122,   0.3122,   0.3122,   0.3122, -38.0971],\n",
      "        [  0.3122,   0.3122,   0.3122, -15.0573, -86.7717],\n",
      "        [  0.3122,   0.3122,   0.3122, -47.2199, -97.9302],\n",
      "        [  0.3122,   0.3122,  -2.0323, -71.8699, -93.4617],\n",
      "        [  0.3122,   0.3122, -34.4764, -84.1252, -93.7422]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0052],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0020, 0.0157],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0069, 0.0201],\n",
      "        [0.0000, 0.0000, 0.0003, 0.0111, 0.0188],\n",
      "        [0.0000, 0.0000, 0.0046, 0.0186, 0.0128]])\n",
      "\n",
      "=== Kernel 14  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[  0.1878,   0.1878,   0.1878,   0.1878,  -6.7635],\n",
      "        [  0.1878,   0.1878,   0.1878,  -3.7146, -30.2212],\n",
      "        [  0.1878,   0.1878,   0.1878,  -4.3690, -50.7149],\n",
      "        [  0.1878,   0.1878,  -0.4075, -15.4559, -70.9687],\n",
      "        [  0.1878,   0.1878,  -7.4993, -62.1593, -95.1177]])\n",
      "IPFE Output:\n",
      " tensor([[  0.1878,   0.1878,   0.1878,   0.1878,  -6.7573],\n",
      "        [  0.1878,   0.1878,   0.1878,  -3.7121, -30.2061],\n",
      "        [  0.1878,   0.1878,   0.1878,  -4.3614, -50.6927],\n",
      "        [  0.1878,   0.1878,  -0.4071, -15.4423, -70.9428],\n",
      "        [  0.1878,   0.1878,  -7.4937, -62.1400, -95.0904]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0061],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0025, 0.0151],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0075, 0.0221],\n",
      "        [0.0000, 0.0000, 0.0004, 0.0135, 0.0259],\n",
      "        [0.0000, 0.0000, 0.0056, 0.0192, 0.0272]])\n",
      "\n",
      "=== Kernel 15  Center 55 Region ===\n",
      "Conv Output:\n",
      " tensor([[ 0.2633,  0.2633,  0.2633,  0.2633, 19.3772],\n",
      "        [ 0.2633,  0.2633,  0.2633,  6.5366, 49.0432],\n",
      "        [ 0.2633,  0.2633,  0.2633, 28.8804, 67.1348],\n",
      "        [ 0.2633,  0.2633,  1.2203, 37.4995, 39.2645],\n",
      "        [ 0.2633,  0.2633, 15.8687, 36.6435,  7.3353]])\n",
      "IPFE Output:\n",
      " tensor([[ 0.2633,  0.2633,  0.2633,  0.2633, 19.3726],\n",
      "        [ 0.2633,  0.2633,  0.2633,  6.5350, 49.0338],\n",
      "        [ 0.2633,  0.2633,  0.2633, 28.8739, 67.1258],\n",
      "        [ 0.2633,  0.2633,  1.2200, 37.4907, 39.2610],\n",
      "        [ 0.2633,  0.2633, 15.8648, 36.6383,  7.3373]])\n",
      "|Difference|:\n",
      " tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0046],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0016, 0.0094],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0065, 0.0090],\n",
      "        [0.0000, 0.0000, 0.0002, 0.0088, 0.0035],\n",
      "        [0.0000, 0.0000, 0.0039, 0.0052, 0.0020]])\n",
      "IPFE predictions: [7]\n",
      "Prediction matches between regular and IPFE: 1/1\n"
     ]
    }
   ],
   "source": [
    "# Test IPFE functionality with a sample\n",
    "def test_ipfe_cnn(model, test_loader, device, num_samples=1):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "    # x is 784 big\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        print(f\"Labels of test samples: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "        # Test regular forward pass\n",
    "        print(\"Testing regular forward pass...\")\n",
    "        regular_outputs = model(images[:num_samples])\n",
    "        _, regular_predicted = regular_outputs.max(1)\n",
    "        print(\"Regular predictions:\", regular_predicted.cpu().numpy())\n",
    "\n",
    "        # Test IPFE forward pass\n",
    "        print(\"Testing IPFE forward pass...\")\n",
    "        try:\n",
    "            ipfe_outputs = model.forward(images[:num_samples], encrypted=True)\n",
    "            _, ipfe_predicted = ipfe_outputs.max(1)\n",
    "\n",
    "            print(f\"IPFE predictions: {ipfe_predicted.cpu().numpy()}\")\n",
    "\n",
    "            # Compare results\n",
    "            matches = (regular_predicted == ipfe_predicted).sum().item()\n",
    "            print(f\"Prediction matches between regular and IPFE: {matches}/{num_samples}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IPFE forward pass failed: {e}\")\n",
    "\n",
    "# Test the IPFE functionality\n",
    "print(\"Testing IPFE-CNN functionality...\")\n",
    "test_ipfe_cnn(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9cc034993ee210",
   "metadata": {},
   "source": [
    "### Old try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b915ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define IPFE-enhanced CNN with functional encryption\n",
    "# class IPFECNN(nn.Module):\n",
    "#     def __init__(self, num_classes=10, prime=104729):\n",
    "#         super(IPFECNN, self).__init__()\n",
    "#         self.prime = prime\n",
    "#         self.ipfe = IPFE(prime)\n",
    "#         self.input_size = None\n",
    "#         self.ipfe.setup(28*28)\n",
    "#         print(\"IPFE setup done\")\n",
    "#\n",
    "#\n",
    "#         # First convolutional block - this will be used with IPFE\n",
    "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
    "#         self.bn1 = nn.BatchNorm2d(16)\n",
    "#         self.pool1 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Second convolutional block\n",
    "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "#         self.bn2 = nn.BatchNorm2d(32)\n",
    "#         self.pool2 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Third convolutional block\n",
    "#         self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "#         self.bn3 = nn.BatchNorm2d(64)\n",
    "#         self.pool3 = nn.MaxPool2d(2, 2)\n",
    "#\n",
    "#         # Fully connected layers\n",
    "#         self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "#         self.fc2 = nn.Linear(128, num_classes)\n",
    "#\n",
    "#         # IPFE setup for first conv layer\n",
    "#         self.ipfe_setup_done = False\n",
    "#\n",
    "#\n",
    "#     def forward(self, x, y_vector=None):\n",
    "#\n",
    "#         # If y_vector is provided, use IPFE for first conv\n",
    "#         if y_vector is not None:\n",
    "#\n",
    "#             if not self.ipfe_setup_done:\n",
    "#                 self.setup_ipfe()\n",
    "#\n",
    "#             self.ipfe.key_derive(y_vector)\n",
    "#             print(f\"IPFE key derivation complete for provided query vector\")\n",
    "#\n",
    "#             # Encrypt the input x (flattened)\n",
    "#             x_flat = x.view(x.size(0), -1).cpu().numpy()\n",
    "#\n",
    "#             # For each sample in the batch, compute IPFE\n",
    "#             ipfe_results = []\n",
    "#             for x_i in x_flat:\n",
    "#                 # Convert input to integers\n",
    "#                 x_int = [int(val * 1000) % (self.prime - 1) for val in x_i]\n",
    "#\n",
    "#                 print(\"<x, y> (expected):\", sum((xi * yi) for xi, yi in zip(x_int, y_vector)) % (self.prime - 1))\n",
    "#\n",
    "#                 # Encrypt input\n",
    "#                 ct = self.ipfe.encrypt(x_int)\n",
    "#\n",
    "#                 # Decrypt to get inner product\n",
    "#                 try:\n",
    "#                     inner_product = self.ipfe.decrypt(ct)\n",
    "#\n",
    "#                     print(\"<x, y> (decrypted):\", inner_product)\n",
    "#\n",
    "#                     ipfe_results.append(inner_product)\n",
    "#                 except:\n",
    "#                     print(\"IPFE decryption failed\")\n",
    "#                     # Fallback to regular computation if IPFE fails\n",
    "#                     inner_product = sum(xi * yi for xi, yi in zip(x_int, y_vector)) % (self.prime - 1)\n",
    "#                     ipfe_results.append(inner_product)\n",
    "#\n",
    "#             # Convert IPFE results back to tensor\n",
    "#             ipfe_tensor = torch.tensor(ipfe_results, dtype=torch.float32).to(x.device)\n",
    "#\n",
    "#         # apply CNN normal\n",
    "#\n",
    "#         x = self.pool1(F.relu(self.bn1(self.conv1(x))))\n",
    "#\n",
    "#         # Continue with remaining layers\n",
    "#         x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "#         x = self.pool3(F.relu(self.bn3(self.conv3(x))))\n",
    "#\n",
    "#         # Flatten and fully connected layers\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.fc2(x)\n",
    "#\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603797d67ee9a810",
   "metadata": {},
   "source": [
    "### run IPFE-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05c5ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPFE setup done, with length: 9\n",
      "weights copied from trained model\n",
      "weights converted to y vectors\n",
      "biases saved\n",
      "sk_ys created\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for IPFECNN:\n\tMissing key(s) in state_dict: \"biases\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m ipfe_model = IPFECNN(num_classes=\u001b[32m10\u001b[39m, prime=\u001b[32m104729\u001b[39m).to(device)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Copy weights from the trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mipfe_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m ipfe_model.setup_ipfe()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPFE-CNN model created on device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for IPFECNN:\n\tMissing key(s) in state_dict: \"biases\". "
     ]
    }
   ],
   "source": [
    "# Initialize IPFE-enhanced CNN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "ipfe_model = IPFECNN(num_classes=10, prime=104729).to(device)\n",
    "\n",
    "# Copy weights from the trained model\n",
    "ipfe_model.load_state_dict(model.state_dict())\n",
    "\n",
    "ipfe_model.setup_ipfe()\n",
    "\n",
    "print(f\"IPFE-CNN model created on device: {device}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in ipfe_model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e2ae2d1d73ad79",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'IPFECNN' object has no attribute 'conv1_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mipfe_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv1_length\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Wehmeyer Konstantin\\OneDrive - Universitaet St.Gallen\\03_3rd Semester\\01_IMP\\PPML_v1\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'IPFECNN' object has no attribute 'conv1_length'"
     ]
    }
   ],
   "source": [
    "print(ipfe_model.conv1_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968c6276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test IPFE functionality with a sample\n",
    "def test_ipfe_cnn(model, test_loader, device, num_samples=5):\n",
    "    \"\"\"Test the IPFE-CNN with a sample query vector\"\"\"\n",
    "    model.eval()\n",
    "    # x is 784 big\n",
    "\n",
    "    # Create a sample query vector y (same length as flattened conv1 weights)\n",
    "    y_vector = [1] * model.conv1_length  # Simple query vector of all 1s\n",
    "    print(\"Using query vector y of length:\", len(y_vector))\n",
    "    with torch.no_grad():\n",
    "        # Get a batch of test data\n",
    "        data_iter = iter(test_loader)\n",
    "        images, labels = next(data_iter)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        print(f\"Labels of test samples: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "        # Test regular forward pass\n",
    "        print(\"Testing regular forward pass...\")\n",
    "        regular_outputs = model(images[:num_samples])\n",
    "        _, regular_predicted = regular_outputs.max(1)\n",
    "        print(\"Regular predictions:\", regular_predicted.cpu().numpy())\n",
    "\n",
    "        # Test IPFE forward pass\n",
    "        print(\"Testing IPFE forward pass...\")\n",
    "        try:\n",
    "            ipfe_outputs = model.forward_with_ipfe(images[:num_samples], y_vector)\n",
    "            _, ipfe_predicted = ipfe_outputs.max(1)\n",
    "\n",
    "            print(f\"Regular predictions: {regular_predicted.cpu().numpy()}\")\n",
    "            print(f\"IPFE predictions: {ipfe_predicted.cpu().numpy()}\")\n",
    "            print(f\"True labels: {labels[:num_samples].cpu().numpy()}\")\n",
    "\n",
    "            # Compare results\n",
    "            matches = (regular_predicted == ipfe_predicted).sum().item()\n",
    "            print(f\"Prediction matches between regular and IPFE: {matches}/{num_samples}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"IPFE forward pass failed: {e}\")\n",
    "\n",
    "# Test the IPFE functionality\n",
    "print(\"Testing IPFE-CNN functionality...\")\n",
    "test_ipfe_cnn(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4356635b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate IPFE with different query vectors\n",
    "def demonstrate_ipfe_queries(model, test_loader, device):\n",
    "    \"\"\"Demonstrate IPFE with different query vectors\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get a single test sample\n",
    "    data_iter = iter(test_loader)\n",
    "    images, labels = next(data_iter)\n",
    "    single_image = images[0:1].to(device)  # Single sample\n",
    "    true_label = labels[0].item()\n",
    "    \n",
    "    print(f\"Testing with image of digit: {true_label}\")\n",
    "    \n",
    "    # Different query vectors to test\n",
    "    query_vectors = {\n",
    "        \"All ones\": [1] * model.conv1_length,\n",
    "        \"All zeros\": [0] * model.conv1_length,\n",
    "        \"Alternating\": [1 if i % 2 == 0 else -1 for i in range(model.conv1_length)],\n",
    "        \"Random\": [1, 0, 1, 0] * (model.conv1_length // 4) + [1] * (model.conv1_length % 4)\n",
    "    }\n",
    "    \n",
    "    print(\"\\nTesting different query vectors:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, y_vector in query_vectors.items():\n",
    "        try:\n",
    "            # Regular forward pass\n",
    "            regular_output = model(single_image)\n",
    "            regular_pred = regular_output.max(1)[1].item()\n",
    "            \n",
    "            # IPFE forward pass\n",
    "            ipfe_output = model.forward_with_ipfe(single_image, y_vector)\n",
    "            ipfe_pred = ipfe_output.max(1)[1].item()\n",
    "            \n",
    "            print(f\"{name:15} - Regular: {regular_pred}, IPFE: {ipfe_pred}, Match: {regular_pred == ipfe_pred}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name:15} - IPFE failed: {str(e)[:50]}...\")\n",
    "\n",
    "# Demonstrate with different queries\n",
    "print(\"Demonstrating IPFE with different query vectors...\")\n",
    "demonstrate_ipfe_queries(ipfe_model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87803409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first CNN layer's filters (weights)\n",
    "def visualize_first_cnn_layer(model):\n",
    "    \"\"\"Visualize the first convolutional layer filters\"\"\"\n",
    "    # Find the first conv layer\n",
    "    first_conv = None\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, torch.nn.Conv2d):\n",
    "            first_conv = m\n",
    "            break\n",
    "    if first_conv is None:\n",
    "        print(\"No Conv2d layer found in model.\")\n",
    "        return\n",
    "\n",
    "    weights = first_conv.weight.data.cpu()\n",
    "    num_filters = weights.shape[0]\n",
    "\n",
    "    # Calculate grid size\n",
    "    cols = 8\n",
    "    rows = (num_filters + cols - 1) // cols\n",
    "\n",
    "    plt.figure(figsize=(cols*1.5, rows*1.5))\n",
    "    for i in range(num_filters):\n",
    "        ax = plt.subplot(rows, cols, i+1)\n",
    "        # For grayscale, show as [out_ch, in_ch, H, W]\n",
    "        w = weights[i, 0].numpy()\n",
    "        ax.imshow(w, cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f'F{i}')\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(\"First Conv Layer Filters (IPFE-enhanced)\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizing first CNN layer filters...\")\n",
    "visualize_first_cnn_layer(ipfe_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcaa144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc540511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc48825",
   "metadata": {},
   "outputs": [],
   "source": []
=======
      "Model saved to models\\cnn_model_1.pth\n"
     ]
    }
   ],
   "execution_count": 15
>>>>>>> main:01_modified_cnn.ipynb
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
